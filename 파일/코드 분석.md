<h1>DiffusionAD(99.700)</h1>
<p>
오토 인코더 기반 네트워크(AE)를 사용하며, 인코더가 입력 이미지를 저차원 표현으로 압축한 후 디코더가 비정상 영역을 정상으로 재구성한다는 가정을 기반으로 한다.
</p>
<p>
문제점<br>
1. 원본 이미지에서 압축된 저차원 표현에는 여전히 비정상적인 정보가 포함되어 있기 때문에 비정상 영역을 불변으로 재구성할 수 있으며, 잘못된 음성 감지가 발생할 수 있다. <br>
2. AE(autoencoderbased networks)는 제한된 복원 기능으로 인해 정상 영역을 거칠게 재구성할 수 있으며, 특히 복잡한 구조 또는 텍스처가 있는 데이터 셋에서 많은 잘못된 양성을 도입할 수 있다. <br><br>

해결책<br>
- 입력 이미지를 교란시키기 위해 가우시안 노이즈 도입<br>
- 추가된 노이즈를 예측하기 위해 노이즈 제거 모델을 사용하여 재구성 프로세스를 노이즈 투 노름 패러다임(noise-to-norm paradigm)으로 재구성<br>
</p>
<p>
무작위로 샘플링된 가우시안 노이즈에서 실제 AD(Anomaly Detection) 시나리오의 실시간 요구 사항보다 훨씬 느리다.<br>
해결책 : 확산 모델을 사용하여 노이즈를 한 번 예측한 다음 재구성 결과를 직접 예측하는 이상 감지를 위한 1단계 노이즈 제거 패러다임을 사용한다. <br>
이렇게 하면 다양한 데이터 셋과 이상 유형의 이상에 걸쳐 원하는 이상 없는 재구성을 달성하는 것을 관찰할 수 있다. 그 후 분할 네트워크는 정확하게 예측 입력과 재구성 간의 불일치 및 공통점을 활용하여 픽셀 수준 이상 점수를 산출한다.
</p>

요약<br>
- 노이즈 투 노름 패러다임을 통해 입력 이미지를 이상 없는 복원으로 재구성하고, 이들 간의 불일치와 공통점을 활용하여 픽셀 단위의 이상 점수를 추가로 예측하는 새로운 파이프라인인 확산 AD를 제안함<br>

데이터 셋(4가지)<br>
1. MVTec(4096개의 정상 이미지와 1258개의 비정상 이미지)<br>
- 스크래치, 균열, 구멍 및 함몰부와 같은 다양한 유형의 표면 결함 샘플이 포함된다.<br>
2. VisA(9621개의 정상 이미지와 1200개의 비정상 이미지)<br>
- 긁힘, 움푹 들어간 곳, 착색된 반점 또는 균열과 같은 표면 결함과 부품의 잘 못 배치 또는 누락과 같은 구조적 결함을 포함하여 다양한 불완전성이 포함된다.<br>
3. DAGM(15000개의 정상 이미지와 2100개의 비정상 이미지)<br>
- 스크래치, 얼룩과 같이 시각적으로 배경에 가까운 다양한 결함이 비정상 샘플을 구성한다.<br>
4. MPDD(1064개의 정상 이미지와 282개의 비정상 영상)<br>
- 금속 가공에 초점을 맞추고 수동으로 작동하는 생산라인에서 마주치는 실제 상황을 반영한다.<br>


결론<br>
비정상적인 영역이 가우시안 노이즈에 의해 교란된 후 다음으로 재구성되는 노이즈 대 노름 패러다임을 채택하는 확산 모델을 사용한 재구성 프로세스이다.<br>
확산 모델의 반복적인 노이즈 제거 접근 방식보다 훨씬 빠른 1단계 노이즈 제거 패러다임을 제시하며, 재구성 품질을 더욱 향상시키기 위해 규범 유도 패러다임을 제안한다.<br>
마지막으로, 분할 하위 네트워크는 불일치와 공통점을 활용한다.<br><br>

```Python
# 주어진 확률 분포로부터 이미지를 샘플링하고, 가우시안 노이즈를 추가하여 샘플링된 이미지를 생성하는 과정
def sample_p(self, model, x_t, t, denoise_fn="gauss"): 
    out = self.p_mean_variance(model, x_t, t) 
    if denoise_fn == "gauss": 
        noise = torch.randn_like(x_t) 
    else:
        noise = denoise_fn(x_t, t)
    nonzero_mask = ( (t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))
    sample = out["mean"] + nonzero_mask * torch.exp(0.5 * out["log_variance"]) * noise 
    return {"sample": sample, "pred_x_0": out["pred_x_0"]}
```
<br><br>
```Python
# 데이터의 노이즈를 제거하여 초기 예측을 개선하고, 이상 감지를 위한 더 강력한 시스템을 구축하기 위한 단계
def norm_guided_one_step_denoising(self, model, x_0, anomaly_label,args):
        normal_t = torch.randint(0, args["less_t_range"], (x_0.shape[0],),device=x_0.device)
        noisier_t = torch.randint(args["less_t_range"],self.num_timesteps,(x_0.shape[0],),device=x_0.device)
        
        normal_loss, x_normal_t, estimate_noise_normal = self.calc_loss(model, x_0, normal_t)
        noisier_loss, x_noiser_t, estimate_noise_noisier = self.calc_loss(model, x_0, noisier_t)
        
        pred_x_0_noisier = self.predict_x_0_from_eps(x_noiser_t, noisier_t, estimate_noise_noisier).clamp(-1, 1)
        pred_x_t_noisier = self.sample_q(pred_x_0_noisier, normal_t, estimate_noise_normal)   

        loss = (normal_loss["loss"]+noisier_loss["loss"])[anomaly_label==0].mean()
        if torch.isnan(loss):
            loss.fill_(0.0)

        estimate_noise_hat = estimate_noise_normal - extract(self.sqrt_one_minus_alphas_cumprod, normal_t, x_normal_t.shape, x_0.device) * args["condition_w"] * (pred_x_t_noisier-x_normal_t)
        pred_x_0_norm_guided = self.predict_x_0_from_eps(x_normal_t, normal_t, estimate_noise_hat).clamp(-1, 1)

        return loss,pred_x_0_norm_guided,normal_t,x_normal_t,x_noiser_t
```
<h1>DRAEM(98.0)</h1>
<p>
DRAEM의 방법은 시뮬레이션된 이상을 사용하여 원본 및 재구성된 공간에 대한 재구성 부분 공간과 초평면을 공동으로 판별적으로 학습함으로써, 실제 이상에 대한 현저한 일반화를 이끌어 낸다.
</p>
<p>
문제점<br>
이상이 없는 데이터에서만 모델을 학습하며, 양성 예시가 훈련 시에 사용되지 않기 때문에, 판별적인 이상 탐지를 명시적으로 최적화하지 않는다. 이상을 학습하기 위해 합성 이상을 고려할 수 있지만, 이는 합성 외관에 과적합되어 실제 이상에 대해 일반화가 잘되지 않는 학습 결정 경계를 만든다.
</p>
<p>
과적합 문제 해결<br>
재구성 부분 공간과 함께 재구성 및 원본 외관을 고려하여 판별적 모델을 훈련한다. 이렇게 하면 모델이 합성 외모에 과적합 되지 않으며, 원본과 재구성된 이상 외관 사이의 지역 외관 조건 거리 함수를 학습하여 다양한 실제 이상에 대해 잘 일반화할 수 있다.
</p>
<p>
DRAEM은 재구성 및 판별적 서브 네트워크로 구성된다.<br>
재구성 서브 네트워크 <br>
- 인코더-디코더 아키텍처로 구성되어 있으며, 입력 이미지의 지역적 패턴을 정상 샘플의 분포에 더 가까운 패턴으로 변환한다.<br>
- 이상을 암시적으로 감지하고 이상이 없는 내용으로 명확하게 재구성을 수행하면서 입력 이미지의 비이상적 영역을 변경하지 않도록 훈련된다.<br><br>
판별 서브 네트워크<br>
- 하위 네트워크는 U-Net과 유사한 구조를 사용한다.<br>
- 공동 재구성 이상 포함을 학습하고 연결된 재구성된 원본 모습에서 정확한 이상 세그멘테이션 맵을 생성한다.<br>
- 이상 훈련 예제는 이상이 없는 이미지에 이상을 시뮬레이션하는 개념적으로 간단한 프로세스를 통해 생성된다. <br>
</p>
<p>
실험<br>
MVTec 이상 탐지 데이터 셋을 기반으로 네트워크를 700epoch 동안 학습 시켰다. 학습률은 10^-4로 설정되었으며, 400번째와 600번째 epoch 이후에는 0.1로 곱해졌다. 학습 중에는 이상이 없는 이미지에 대해 (-45,45)도의 범위 내에서 이미지 회전을 데이터 증가 방법으로 사용하여, 상대적으로 작은 이상이 없는 학습 세트 크기로 인한 과적합을 완하하였다. DRAEM은 이상이 없는 훈련 샘플만 사용하여 동일한 매개변수를 사용하여 훈련된다. 이 데이터 셋에서의 표준평가 프로토콜은 이미지에 이상이 포함되어 있는지 여부를 분류하는 것이다. 이상의 정확한 위치는 측정되지 않으며, 이상은 대략적으로만 레이블이 지정된다. 
</p> 
<p>
결론<br>
DRAEM은 엔드 투 엔드 훈련 가능한 표면 이상 탐지 및 위치 지정 방법이다. DAGM 데이터셋에서 DRAEM은 완전히 감독된 방법에 근접한 이상 이미지 분류 정확도를 제공하며, 위치 지정 정확도에서 능가한다. 이는 DRAEM이 실제 이상에 대해 훈련되지 않았음에도 불구하고 현저한 결과이다.
</p>
```Python
# 입력 이미지를 재구성하는 서브 네트워크 정의
class ReconstructiveSubNetwork(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, base_width=128):
        super(ReconstructiveSubNetwork, self).__init__()
        self.encoder = EncoderReconstructive(in_channels, base_width)
        self.decoder = DecoderReconstructive(base_width, out_channels=out_channels)

    def forward(self, x):
        b5 = self.encoder(x)
        output = self.decoder(b5)
        return output
```
<br><br>
```Python
# 판별적 서브 네트워크 정의
class DiscriminativeSubNetwork(nn.Module):
    def __init__(self,in_channels=3, out_channels=3, base_channels=64, out_features=False):
        super(DiscriminativeSubNetwork, self).__init__()
        base_width = base_channels
        self.encoder_segment = EncoderDiscriminative(in_channels, base_width)
        self.decoder_segment = DecoderDiscriminative(base_width, out_channels=out_channels)
        #self.segment_act = torch.nn.Sigmoid()
        self.out_features = out_features

    def forward(self, x):
        b1,b2,b3,b4,b5,b6 = self.encoder_segment(x)
        output_segment = self.decoder_segment(b1,b2,b3,b4,b5,b6)
        if self.out_features:
            return output_segment, b2, b3, b4, b5, b6
        else:
            return output_segment
```
	    
