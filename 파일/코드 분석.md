<h1>DiffusionAD(99.70)</h1>
<p>
오토 인코더 기반 네트워크(AE)를 사용하며, 인코더가 입력 이미지를 저차원 표현으로 압축한 후 디코더가 비정상 영역을 정상으로 재구성한다는 가정을 기반으로 한다.
</p>
<p>
문제점<br>
1. 이미지를 낮은 차원의 형태로 압축할 때 원래의 데이터에는 없던 비정상적인 정보가 남을 수 있다. 이 비정상적인 정보가 남아 있는 상황에서 원래의 데이터를 재구성할 때 비정상적인 정보가 그대로 나타날 수 있다. 그래서 잘못된 정보를 포함한 데이터를 정확하게 처리하기 어려울 수 있다. <br>
2. AEs(autoencoderbased networks)가 정상 영역을 정확하게 재구성하지 못할 수도 있다. 특히 복잡한 구조나 텍스처가 있는 데이터에서는 이러한 문제가 더 자주 발생할 수도 있다. 그래서 정상적인 것으로 여겨지는 것조차도 부정확하게 재구성할 수 있고, 이로 인해 잘못된 결과를 낼 수 있다. <br><br>

해결책<br>
- 입력 이미지를 교란시키기 위해 가우시안 노이즈 도입<br>
- 추가된 노이즈를 예측하기 위해 노이즈 제거 모델을 사용하여 재구성 프로세스를 노이즈 투 노름 패러다임(noise-to-norm paradigm)으로 재구성<br>
</p>
<p>
무작위로 샘플링된 가우시안 노이즈에서 실제 이상 감지 시나리오의 실시간 요구 사항보다 훨씬 느리다.<br>
해결책 : 확산 모델을 사용하여 노이즈를 예측하고 제거한 후, 이 노이즈가 제거된 데이터를 사용하여 재구성을 수행하고, 재구성된 결과를 이용하여 이상을 직접 예측하는 1단계 노이즈 제거 패러다임을 사용한다. <br>
</p>

요약<br>
- 노이즈 투 노름 패러다임을 통해 입력 이미지를 이상 없는 복원으로 재구성하고, 이들 간의 불일치와 공통점을 활용하여 픽셀 단위의 이상 점수를 추가로 예측하는 새로운 파이프라인인 확산 AD를 제안함<br>

결론<br>
입력 이미지의 비정상적인 영역이 가우시안 노이즈에 의해 교란된 후, 노이즈 투 노름 패러다임을 따르는 확산 모델을 사용한 재구성된다. 이는 확산 모델의 반복적인 노이즈 제거보다 빠르며, 재구성 품질을 향상시키기 위해 규범 유도 패러다임이 제안된다. 마지막으로, 분할 하위 네트워크는 불일치와 공통점을 활용하여 작동한다. <br><br>

```Python
# 퍼린 잡음을 생성하는 함수로 이미지를 교란시키는 코드
def rand_perlin_2d_np(shape, res, fade=lambda t: 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3): 
    delta = (res[0] / shape[0], res[1] / shape[1]) 
    d = (shape[0] // res[0], shape[1] // res[1])  
    grid = np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1 

    angles = 2 * math.pi * np.random.rand(res[0] + 1, res[1] + 1)    
    gradients = np.stack((np.cos(angles), np.sin(angles)), axis=-1) 
    tt = np.repeat(np.repeat(gradients,d[0],axis=0),d[1],axis=1) 

    tile_grads = lambda slice1, slice2: np.repeat(np.repeat(gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]],d[0],axis=0),d[1],axis=1)
    dot = lambda grad, shift: (
                np.stack((grid[:shape[0], :shape[1], 0] + shift[0], grid[:shape[0], :shape[1], 1] + shift[1]),
                            axis=-1) * grad[:shape[0], :shape[1]]).sum(axis=-1)

    n00 = dot(tile_grads([0, -1], [0, -1]), [0, 0]) 
    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])
    n01 = dot(tile_grads([0, -1], [1, None]), [0, -1])
    n11 = dot(tile_grads([1, None], [1, None]), [-1, -1])
    t = fade(grid[:shape[0], :shape[1]]) #(256,256,2)
    return math.sqrt(2) * lerp_np(lerp_np(n00, n10, t[..., 0]), lerp_np(n01, n11, t[..., 0]), t[..., 1]) #(256,256)

```
```Python
# 노이즈 예측해서 제거
def norm_guided_one_step_denoising(self, model, x_0, anomaly_label,args):
        normal_t = torch.randint(0, args["less_t_range"], (x_0.shape[0],),device=x_0.device)
        noisier_t = torch.randint(args["less_t_range"],self.num_timesteps,(x_0.shape[0],),device=x_0.device)
        
        normal_loss, x_normal_t, estimate_noise_normal = self.calc_loss(model, x_0, normal_t)
        noisier_loss, x_noiser_t, estimate_noise_noisier = self.calc_loss(model, x_0, noisier_t)
        
        pred_x_0_noisier = self.predict_x_0_from_eps(x_noiser_t, noisier_t, estimate_noise_noisier).clamp(-1, 1)
        pred_x_t_noisier = self.sample_q(pred_x_0_noisier, normal_t, estimate_noise_normal)   

        loss = (normal_loss["loss"]+noisier_loss["loss"])[anomaly_label==0].mean()
        if torch.isnan(loss):
            loss.fill_(0.0)

        estimate_noise_hat = estimate_noise_normal - extract(self.sqrt_one_minus_alphas_cumprod, normal_t, x_normal_t.shape, x_0.device) * args["condition_w"] * (pred_x_t_noisier-x_normal_t)
        pred_x_0_norm_guided = self.predict_x_0_from_eps(x_normal_t, normal_t, estimate_noise_hat).clamp(-1, 1)

        return loss,pred_x_0_norm_guided,normal_t,x_normal_t,x_noiser_t
```
![image](https://github.com/kimgang9/SW_development/assets/163000187/fd38c4e0-a6c1-497e-a793-1c287107c40c)
<br>
<p>
과정<br>
- 데이터임베딩 : 입력 데이터를 저차원의 특성 공간으로 임베딩한다. 이를 통해 데이터의 특성을 보존하면서 차원을 줄이고, 데이터의 복잡성을 감소시킨다.<br>
- 확산 : 임베딩된 데이터를 확산하여 주변 이웃들과의 관계를 모델링한다. 이 과정은 데이터 포인트 간의 유사성을 측정하고, 이웃 데이터간의 정보 전파를 수행하여 데이터의 구조와 패턴을 보다 잘 파악할 수 있도록 한다.<br>
- 이상 점수 계산 : 확산된 데이터 표현을 기반으로 각 데이터 포인트에 대한 이상 점수를 계산한다.<br>
- 이상 탐지 : 계산된 이상 점수를 기준으로 이상을 탐지한다. 미리 정의된 임계값을 초과하는 경우 해당 데이터 포인트를 이상으로 분류한다.<br>
</p>

<h1>DRAEM(98.0)-Discriminatively trained Reconstruction Anomaly Embedding Model</h1>
<p>
DRAEM 이 모델은 시뮬레이션된 이상을 사용하여 원본 데이터와 재구성된 데이터의 공간을 분석하고, 이를 통해 재구성된 데이터와 원본 데이터간의 차이를 설명하는 부분 공간과 초평면을 학습한다.
</p>
<p>
문제점<br>
정상 데이터만을 사용하여 훈련되어, 이상을 명시적으로 구분하는데 초점을 두지 않는다. 때때로 합성된 이상 데이터를 사용할 수 있지만, 이는 모델이 합성된 이상에만 적합하게 학습되는 과적합 현상이 일어나 실제 이상을 잘 파악하지 못할 수 있다.
</p>
<p>
과적합 문제 해결<br>
원본과 재구성된 데이터의 외관 차이를 고려해서 학습하여, 합성된 외모에 과적합되지 않고 다양한 실제 이상을 잘 탐지할 수 있도록 돕는다. 
</p>
<p>
DRAEM은 재구성 서브 네트워크와 판별적 서브 네트워크로 구성된다.<br>
재구성 서브 네트워크 <br>
- 재구성 서브 네트워크는 인코더와 디코더로 이루어진 구조이다. 이 네트워크는 입력된 이미지를 분석하여 그 이미지가 정상적인 경우에 가지는 특징과 비슷한 패턴을 만들어 낸다.<br>
- 정상적인 이미지를 잘 복원하면서 이상이 있는 부분을 그대로 유지하려고 한다.<br><br>
판별적 서브 네트워크<br>
- 판별적 서브 네트워크는 U-Net과 유사한 구조를 사용한다.<br>
- 이상을 포함한 재구성을 학습하고, 재구성된 원본 이미지에서 이상이 있는 부분을 정확하게 식별할 수 있는 맵인 세그멘테이션 맵을 생성한다.<br>
</p>
<p>
결론<br>
DRAEM은 엔드 투 엔드로 훈련할 수 있는 이상을 탐지하고 위치를 찾는 방법이다. DAGM 데이터셋에서 이 모델은 완전히 감독된 방법과 비슷한 정확도를 제공하며, 위치를 정확하게 찾는데 더 뛰어나다. 이는 DRAEM이 실제 이상을 사용하여 훈련되지 않았지만 좋은 결과를 내놓는다는 것을 의미한다. 
</p>

```Python
# 입력 이미지를 재구성하는 서브 네트워크 정의
class ReconstructiveSubNetwork(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, base_width=128):
        super(ReconstructiveSubNetwork, self).__init__()
        self.encoder = EncoderReconstructive(in_channels, base_width)
        self.decoder = DecoderReconstructive(base_width, out_channels=out_channels)

    def forward(self, x):
        b5 = self.encoder(x)
        output = self.decoder(b5)
        return output
```
```Python
# 판별적 서브 네트워크 정의
class DiscriminativeSubNetwork(nn.Module):
    def __init__(self,in_channels=3, out_channels=3, base_channels=64, out_features=False):
        super(DiscriminativeSubNetwork, self).__init__()
        base_width = base_channels
        self.encoder_segment = EncoderDiscriminative(in_channels, base_width)
        self.decoder_segment = DecoderDiscriminative(base_width, out_channels=out_channels)
        #self.segment_act = torch.nn.Sigmoid()
        self.out_features = out_features

    def forward(self, x):
        b1,b2,b3,b4,b5,b6 = self.encoder_segment(x)
        output_segment = self.decoder_segment(b1,b2,b3,b4,b5,b6)
        if self.out_features:
            return output_segment, b2, b3, b4, b5, b6
        else:
            return output_segment
```
![image](https://github.com/kimgang9/SW_development/assets/163000187/3dd5e6a1-2613-4b92-8f7a-53d66d574239)
<br>
<p>
과정<br>
- simulator를 통한 이상 이미지 생성 : 정상이미지를 변형하여 다양한 종류의 이상 상황을 시뮬레이션한다.<br>
- 재구성 및 분할 학습 : 생성된 이상 이미지를 원본 정상 이미지로 복원하고, 이상 마스크를 분할하는 두가지 태스크를 통해 모델을 학습한다.<br>
- 오토인코더와 판별자의 결합 : 오토인코더와 판별자를 함께 사용하여 이상 이미지와 정상 이미지의 특징을 함께 학습한다. 이를 통해 더 강력한 이상 탐지를 수행할 수 있다.<br>
</p>

<h1>CPR(99.70)-Cascade Patch Retrieval</h1>
<p>
이 모델은 캐스케이드 패치 검색 절차를 사용하여 작은 이미지 부분을 하나씩 검사하여 이상을 찾아낸다. 검사는 패치들 사이에서 인접한 것들을 조금씩 더 디테일하게 확인하면서 진행된다.<br>
1. 주어진 테스트 샘플에서 강력한 히스토그램 매칭을 사용하여 가장 비슷한 훈련 이미지를 찾는다. 이 과정에서 상위 K개의 유사한 훈련 이미지를 선택한다.<br>
2. 각 테스트 패치의 최근접 이웃은 유사한 위치에 있는 다른 패치들을 사용하여 훈련된 지역 메트릭을 기반으로 신중하게 검색된다.<br>
3. 각 테스트 패치의 이상 점수는 패치와 그 근처 패치들간의 거리, 패치가 배경이 아닌 객체를 포함할 확률 이 두가지를 고려하여 계산된다.<br>
또한, 검색을 할 때 목표 및 사격(target & shoot) 방식을 사용한다. 이는 테스트 이미지를 직접 검색하는 대신, 각 테스트 패치에 대해 적절한 참조 목표를 선택한다. 이 목표는 테스트 패치와 유사한 이미지 위치에 있어야하며, 전역 검색 단계에서 얻은 참조 이미지에서만 가져와야 한다.
</p>
<p>
요약<br>
- 패치 뱅크를 무작위로 검색하거나 테스트 이미지를 특정 자세에 맞추는 대신, 카스케이드 검색 전략을 사용한다. 이 방법은 검색을 단계적으로 수행하여 높은 검색 회수와 더 나은 검색 매트릭을 학습할 충분한 공간을 제공한다.<br>
- 대부분의 AD 데이터 셋에서 소수의 샘플 조건 하에서의 과적합 문제를 해결하기 위해, 맞춤형 대조 손실과 더 보수적인 학습 전략을 가진 새로운 메트릭 학습 프레임워크를 제안한다.<br>
</p>
<p>
CPR 알고리즘의 세부 사항<br>
CNN 기반의 CPR 모델은 DenseNet201 백본, 전역 검색 분기(GRB), 지역 검색 분기(LRB) 및 전경 추정 분기(FEB)라는 4개의 하위 네트워크로 구성된다.
</p>

```Python
# 전경 추정 분기(FEB)
class ForegroundEstimateBranch(nn.Module):
    def __init__(self, in_channels: int) -> None:
        super().__init__()
        # 1x1 convolution layer to reduce channels to 1
        self.conv1x1 = nn.Conv2d(in_channels, 1, 1, 1).requires_grad_(False)
    
    def initialize_weights(self, lda: LinearDiscriminantAnalysis, normalizer: MinMaxScaler):
        # Initialize weights and bias using coefficients and intercept from LDA model
        self.conv1x1.weight.data = torch.from_numpy(lda.coef_.T).float()[None, :, :, None] / torch.tensor(normalizer.scale_).float()
        self.conv1x1.bias.data = (torch.from_numpy(lda.intercept_).float() - torch.tensor(normalizer.min_).float()) / torch.tensor(normalizer.scale_).float()
        return self
    
    @torch.no_grad()
    def forward(self, x):
        # Forward pass through the 1x1 convolution layer
        return torch.clamp(self.conv1x1(x), 0, 1)
```
```Python
# 지역 검색 분기
class LocalRetrievalBranch(nn.Module):
    def __init__(self, in_channels_list: List[int], out_channels_list: List[int]) -> None:
        super().__init__()
        self.in_channels_list = in_channels_list
        self.out_channels_list = out_channels_list
        self.conv = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, 192, kernel_size=1),
                nn.BatchNorm2d(192),
                nn.ReLU(inplace=True),
                Inception(192, 256),
                nn.Conv2d(256, out_channels, kernel_size=1)
            ) for in_channels, out_channels in zip(in_channels_list, out_channels_list)
        ])
    
    def forward(self, xs: List[torch.Tensor]):
        assert len(xs) == len(self.conv), f'input length must match conv num: {len(xs)} vs {len(self.conv)}'
        return [layer(x) for x, layer in zip(xs, self.conv)]
```
```Python
# 전역 검색 분기
class GlobalRetrievalBranch(nn.Module):
    def __init__(self, bank_size, input_size, in_channels, n_clusters, S, d_method='kl', l_ratio=4/5) -> None:
        super().__init__()
        self.l_ratio = l_ratio
        self.d_method = d_method
        self.codebook = Codebook(n_clusters, in_channels)
        self.block_wise_histogram_encoder = BlockWiseHistogramEncoder(S, input_size, n_clusters)
        self.refs = nn.Parameter(torch.zeros(bank_size, S**2, n_clusters), requires_grad=False)
        self.register_buffer('_bank', torch.tensor(False))

    def initialize_weights(self, kmeans: KMeans):
        self.codebook.initialize_weights(kmeans)
        return self
    
    @torch.no_grad()
    def forward(self, x, return_code = False):
        code = self.codebook(x)
        x = self.block_wise_histogram_encoder(code)
        if return_code:
            return x, code
        return x

    def set_bank(self, refs):
        self.refs.data = refs
        self._bank = torch.tensor(True, device=refs.device)

    def retrieval(self, query):
        assert self._bank, f'GlobalRetrievalBranch must set bank before retrieval.'
        if self.d_method == 'kl':
            idx = torch.argsort(torch.sort(entropy_pytorch(
                        query + 1e-8, self.refs + 1e-8, -1
                    ), -1)[0][:, :int(query.shape[1] * self.l_ratio)].sum(-1))
        else:
            idx = torch.argsort(torch.sort(torch.norm(
                        query - self.refs, dim=-1
                    ), -1)[0][:, :int(query.shape[1] * self.l_ratio)].sum(-1))
        return idx
```
```Python
# DenseNet201 백본 생성
def create_model(model_name: str = 'DenseNet', layers: List[str] = ['features.denseblock1', 'features.denseblock2'], input_size: int = 320, output_dim: int = 384) -> CPR:
    backbone: BaseModel = MODEL_INFOS[model_name]['cls'](layers, input_size=input_size).eval()
    lrb = LocalRetrievalBranch([shape[1] for shape in backbone.feature_extractor.shapes.values()], [output_dim] * len(layers))
    return CPR(backbone, lrb)
```
![image](https://github.com/kimgang9/SW_development/assets/163000187/ac0b85c1-8d72-4d40-82f7-9091d9c3d94a)
<br>
<p>
과정<br>
- 이미지 특성 추출 : 입력 이미지에서 중요한 특징을 추출한다.<br>
- 이상 패치 검색 : 추출된 특성을 기반으로 이상을 감지하기 위해 이상 패치를 직접적으로 찾는다.<br>
- 이상 점수 계산 : 검색된 이상 패치를 기반으로 이미지 전체에 대한 이상 점수를 계산한다.<br>
- 이상 탐지 : 이상 점수가 임계값을 초과하는 경우 해당 이미지를 이상으로 분류한다.<br>
</p>

