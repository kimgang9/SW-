<h1>DiffusionAD(99.70)</h1>
<p>
오토 인코더 기반 네트워크(AE)를 사용하며, 인코더가 입력 이미지를 저차원 표현으로 압축한 후 디코더가 비정상 영역을 정상으로 재구성한다는 가정을 기반으로 한다.
</p>
<p>
문제점<br>
1. 이미지를 낮은 차원의 형태로 압축할 때 원래의 데이터에는 없던 비정상적인 정보가 남을 수 있다. 이 비정상적인 정보가 남아 있는 상황에서 원래의 데이터를 재구성할 때 비정상적인 정보가 그대로 나타날 수 있다. 그래서 잘못된 정보를 포함한 데이터를 정확하게 처리하기 어려울 수 있다. <br>
2. AEs(autoencoderbased networks)가 정상 영역을 정확하게 재구성하지 못할 수도 있다. 특히 복잡한 구조나 텍스처가 있는 데이터에서는 이러한 문제가 더 자주 발생할 수도 있다. 그래서 정상적인 것으로 여겨지는 것조차도 부정확하게 재구성할 수 있고, 이로 인해 잘못된 결과를 낼 수 있다. <br><br>

해결책<br>
- 입력 이미지를 교란시키기 위해 가우시안 노이즈 도입<br>
- 추가된 노이즈를 예측하기 위해 노이즈 제거 모델을 사용하여 재구성 프로세스를 노이즈 투 노름 패러다임(noise-to-norm paradigm)으로 재구성<br>
</p>
<p>
무작위로 샘플링된 가우시안 노이즈에서 실제 이상 감지 시나리오의 실시간 요구 사항보다 훨씬 느리다.<br>
해결책 : 확산 모델을 사용하여 노이즈를 예측하고 제거한 후, 이 노이즈가 제거된 데이터를 사용하여 재구성을 수행하고, 재구성된 결과를 이용하여 이상을 직접 예측하는 1단계 노이즈 제거 패러다임을 사용한다. <br>
</p>

요약<br>
- 노이즈 투 노름 패러다임을 통해 입력 이미지를 이상 없는 복원으로 재구성하고, 이들 간의 불일치와 공통점을 활용하여 픽셀 단위의 이상 점수를 추가로 예측하는 새로운 파이프라인인 확산 AD를 제안함<br>

데이터 셋(4가지)<br>
1. MVTec(4096개의 정상 이미지와 1258개의 비정상 이미지)<br>
- 스크래치, 균열, 구멍 및 함몰부와 같은 다양한 유형의 표면 결함 샘플이 포함된다.<br>
2. VisA(9621개의 정상 이미지와 1200개의 비정상 이미지)<br>
- 긁힘, 움푹 들어간 곳, 착색된 반점 또는 균열과 같은 표면 결함과 부품의 잘 못 배치 또는 누락과 같은 구조적 결함을 포함하여 다양한 불완전성이 포함된다.<br>
3. DAGM(15000개의 정상 이미지와 2100개의 비정상 이미지)<br>
- 스크래치, 얼룩과 같이 시각적으로 배경에 가까운 다양한 결함이 비정상 샘플을 구성한다.<br>
4. MPDD(1064개의 정상 이미지와 282개의 비정상 영상)<br>
- 금속 가공에 초점을 맞추고 수동으로 작동하는 생산라인에서 마주치는 실제 상황을 반영한다.<br>


결론<br>
입력 이미지의 비정상적인 영역이 가우시안 노이즈에 의해 교란된 후, 노이즈 투 노름 패러다임을 따르는 확산 모델을 사용한 재구성된다. 이는 확산 모델의 반복적인 노이즈 제거보다 빠르며, 재구성 품질을 향상시키기 위해 규범 유도 패러다임이 제안된다. 마지막으로, 분할 하위 네트워크는 불일치와 공통점을 활용하여 작동한다. <br><br>

```Python
# 퍼린 잡음을 생성하는 함수로 이미지를 교란시키는 코드
def rand_perlin_2d_np(shape, res, fade=lambda t: 6 * t ** 5 - 15 * t ** 4 + 10 * t ** 3): 
    delta = (res[0] / shape[0], res[1] / shape[1]) 
    d = (shape[0] // res[0], shape[1] // res[1])  
    grid = np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1 

    angles = 2 * math.pi * np.random.rand(res[0] + 1, res[1] + 1)    
    gradients = np.stack((np.cos(angles), np.sin(angles)), axis=-1) 
    tt = np.repeat(np.repeat(gradients,d[0],axis=0),d[1],axis=1) 

    tile_grads = lambda slice1, slice2: np.repeat(np.repeat(gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]],d[0],axis=0),d[1],axis=1)
    dot = lambda grad, shift: (
                np.stack((grid[:shape[0], :shape[1], 0] + shift[0], grid[:shape[0], :shape[1], 1] + shift[1]),
                            axis=-1) * grad[:shape[0], :shape[1]]).sum(axis=-1)

    n00 = dot(tile_grads([0, -1], [0, -1]), [0, 0]) 
    n10 = dot(tile_grads([1, None], [0, -1]), [-1, 0])
    n01 = dot(tile_grads([0, -1], [1, None]), [0, -1])
    n11 = dot(tile_grads([1, None], [1, None]), [-1, -1])
    t = fade(grid[:shape[0], :shape[1]]) #(256,256,2)
거
def norm_guided_one_step_denoising(self, model, x_0, anomaly_label,args):
        normal_t = torch.randint(0, args["less_t_range"], (x_0.shape[0],),device=x_0.device)
        noisier_t = torch.randint(args["less_t_range"],self.num_timesteps,(x_0.shape[0],),device=x_0.device)
        
        normal_loss, x_normal_t, estimate_noise_normal = self.calc_loss(model, x_0, normal_t)
        noisier_loss, x_noiser_t, estimate_noise_noisier = self.calc_loss(model, x_0, noisier_t)
        
        pred_x_0_noisier = self.predict_x_0_from_eps(x_noiser_t, noisier_t, estimate_noise_noisier).clamp(-1, 1)
        pred_x_t_noisier = self.sample_q(pred_x_0_noisier, normal_t, estimate_noise_normal)   

        loss = (normal_loss["loss"]+noisier_loss["loss"])[anomaly_label==0].mean()
        if torch.isnan(loss):
            loss.fill_(0.0)

        estimate_noise_hat = estimate_noise_normal - extract(self.sqrt_one_minus_alphas_cumprod, normal_t, x_normal_t.shape, x_0.device) * args["condition_w"] * (pred_x_t_noisier-x_normal_t)
        pred_x_0_norm_guided = self.predict_x_0_from_eps(x_normal_t, normal_t, estimate_noise_hat).clamp(-1, 1)

        return loss,pred_x_0_norm_guided,normal_t,x_normal_t,x_noiser_t
```
<h1>DRAEM(98.0)</h1>
<p>
DRAEM의 방법은 시뮬레이션된 이상을 사용하여 원본 및 재구성된 공간에 대한 재구성 부분 공간과 초평면을 공동으로 판별적으로 학습함으로써, 실제 이상에 대한 현저한 일반화를 이끌어 낸다.
</p>
<p>
문제점<br>
이상이 없는 데이터에서만 모델을 학습하며, 양성 예시가 훈련 시에 사용되지 않기 때문에, 판별적인 이상 탐지를 명시적으로 최적화하지 않는다. 이상을 학습하기 위해 합성 이상을 고려할 수 있지만, 이는 합성 외관에 과적합되어 실제 이상에 대해 일반화가 잘되지 않는 학습 결정 경계를 만든다.
</p>
<p>
과적합 문제 해결<br>
재구성 부분 공간과 함께 재구성 및 원본 외관을 고려하여 판별적 모델을 훈련한다. 이렇게 하면 모델이 합성 외모에 과적합 되지 않으며, 원본과 재구성된 이상 외관 사이의 지역 외관 조건 거리 함수를 학습하여 다양한 실제 이상에 대해 잘 일반화할 수 있다.
</p>
<p>
DRAEM은 재구성 및 판별적 서브 네트워크로 구성된다.<br>
재구성 서브 네트워크 <br>
- 인코더-디코더 아키텍처로 구성되어 있으며, 입력 이미지의 지역적 패턴을 정상 샘플의 분포에 더 가까운 패턴으로 변환한다.<br>
- 이상을 암시적으로 감지하고 이상이 없는 내용으로 명확하게 재구성을 수행하면서 입력 이미지의 비이상적 영역을 변경하지 않도록 훈련된다.<br><br>
판별 서브 네트워크<br>
- 하위 네트워크는 U-Net과 유사한 구조를 사용한다.<br>
- 공동 재구성 이상 포함을 학습하고 연결된 재구성된 원본 모습에서 정확한 이상 세그멘테이션 맵을 생성한다.<br>
- 이상 훈련 예제는 이상이 없는 이미지에 이상을 시뮬레이션하는 개념적으로 간단한 프로세스를 통해 생성된다. <br>
</p>
<p>
실험<br>
MVTec 이상 탐지 데이터 셋을 기반으로 네트워크를 700epoch 동안 학습 시켰다. 학습률은 10^-4로 설정되었으며, 400번째와 600번째 epoch 이후에는 0.1로 곱해졌다. 학습 중에는 이상이 없는 이미지에 대해 (-45,45)도의 범위 내에서 이미지 회전을 데이터 증가 방법으로 사용하여, 상대적으로 작은 이상이 없는 학습 세트 크기로 인한 과적합을 완하하였다. DRAEM은 이상이 없는 훈련 샘플만 사용하여 동일한 매개변수를 사용하여 훈련된다. 이 데이터 셋에서의 표준평가 프로토콜은 이미지에 이상이 포함되어 있는지 여부를 분류하는 것이다. 이상의 정확한 위치는 측정되지 않으며, 이상은 대략적으로만 레이블이 지정된다. 
</p> 
<p>
결론<br>
DRAEM은 엔드 투 엔드 훈련 가능한 표면 이상 탐지 및 위치 지정 방법이다. DAGM 데이터셋에서 DRAEM은 완전히 감독된 방법에 근접한 이상 이미지 분류 정확도를 제공하며, 위치 지정 정확도에서 능가한다. 이는 DRAEM이 실제 이상에 대해 훈련되지 않았음에도 불구하고 현저한 결과이다.
</p>

```Python
# 입력 이미지를 재구성하는 서브 네트워크 정의
class ReconstructiveSubNetwork(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, base_width=128):
        super(ReconstructiveSubNetwork, self).__init__()
        self.encoder = EncoderReconstructive(in_channels, base_width)
        self.decoder = DecoderReconstructive(base_width, out_channels=out_channels)

    def forward(self, x):
        b5 = self.encoder(x)
        output = self.decoder(b5)
        return output
```
<br><br>
```Python
# 판별적 서브 네트워크 정의
class DiscriminativeSubNetwork(nn.Module):
    def __init__(self,in_channels=3, out_channels=3, base_channels=64, out_features=False):
        super(DiscriminativeSubNetwork, self).__init__()
        base_width = base_channels
        self.encoder_segment = EncoderDiscriminative(in_channels, base_width)
        self.decoder_segment = DecoderDiscriminative(base_width, out_channels=out_channels)
        #self.segment_act = torch.nn.Sigmoid()
        self.out_features = out_features

    def forward(self, x):
        b1,b2,b3,b4,b5,b6 = self.encoder_segment(x)
        output_segment = self.decoder_segment(b1,b2,b3,b4,b5,b6)
        if self.out_features:
            return output_segment, b2, b3, b4, b5, b6
        else:
            return output_segment
```
<h1>CPR(99.70)</h1>
<p>
이 모델은 이상 탐지 모델을 세부적인 단계로 접근하여 각 테스트 이미지 패치에 대해 최근접 이웃을 점점 더 세밀한 단계로 순회하며 검색하는 카스케이드 패치 검색 절차를 통해 해결된다.<br>
1. 주어진 테스트 샘플에서 강력한 히스토그램 매칭 과정을 기반으로 최상위 K 개의 가장 유사한 훈련 이미지가 선택된다.<br>
2. 각 테스트 패치의 최근접 이웃은 이러한 전역 최근접 이웃에서 유사한 기하적 위치에 있는 것들을 사용하여 신중하게 훈련된 지역 메트릭을 사용하여 검색된다.<br>
3. 각 테스트 패치의 이상 점수는 지역 최근접 이웃까지의 거리와 비 배경(non-background) 확률에 기반하여 계산된다.<br>
또한, 목표 및 사격(target & shoot) 방식으로 검색을 수행한다. 테스트 이미지를 정렬하는 대신에 각 테스트 패치에 대해 적절한 참조 목표를 선택한다.<br>
구체적으로, 자격이 있는 목표 샘플은 테스트 패치와 유사한 이미지 좌표에 위치해야 하며 전역 검색 단계에서 얻은 참조 이미지에서만 추출되어야 한다.<br>
</p>
<p>
요약<br>
- 이상 탐지 작업을 카스케이드 검색 문제로 캐스팅한다. 패치 뱅크를 무차별적으로 검색하거나 테스트 이미지를 표준 자세에 맞추는 대신, 카스케이드 검색 전략은 자연스럽게 높은 검색 회수를 가지며 더 나은 검색 메트릭을 학습할 충분한 공간을 제공한다.<br>
- 대부분의 AD 데이터 셋에서 소수의 샘플 조건 하에서의 과적합 문제를 해결하기 위해, 맞춤형 대조 손실과 더 보수적인 학습 전략을 가진 새로운 메트릭 학습 프레임워크를 제안한다.<br>
</p>
<p>
CPR 알고리즘의 세부 사항<br>
CNN 기반의 CPR 모델은 DenseNet201 백본, 전역 검색 분기(GRB), 지역 검색 분기(LRB) 및 전경 추정 분기(FEB)라는 4개의 하위 네트워크로 구성된다.
</p>
<p>
구현 세부 정보<br>
입력 이미지는 훈련 및 테스트 과정에서 모두 320 × 320 크기로 일관되게 조정된다. ImageNet에서 사전 훈련된 DenseNet201 모델의 "denseblock-1" 및 "denseblock-2"를 사용하여 크기가 각각 256×80×80 및 512×40×40인 원시 특성 텐서를 얻고 이 블록들을 훈련 중에 고정한다. 특히, GRB와 FEB는 denseblock-1을 기반으로만 수행되며 LRB는 다중 스케일로 denseblock-1과 denseblock-2를 모두 사용한다. 로컬 검색을 위한 특성 공간의 차원은 384이다. 추론 단계에서 K 최근접 이웃의 수는 10으로 설정되며, 하위 텐서 S의 크기는 5로 설정되며, 클러스터 수 Nc는 12로 설정된다. 강력한 KL 발산을 위한 매개 변수 τ는 5로 설정되며, 이미지 수준의 이상치 점수를 계산하는 데 사용되는 매개 변수 T는 512이다. 또한 denseblock-1 및 denseblock-2의 로컬 검색 영역 크기는 각각 3 및 1로 설정된다. 훈련 설정으로, AdamW 옵티마이저를 사용하여 모델 매개 변수를 업데이트하고 기본 학습률 1×10^−3 및 기본 가중치 감쇠율 1×10^−2를 사용한다. 각 하위 범주에 대해 배치 크기가 32 인 40000 번의 반복을 통해 진행되었으며, 한 데이터셋의 모든 하위 범주에 대해 경험적으로 고정된 반복 횟수를 선택했다. 
</p>

```Python
# 전경 추정 분기(FEB)
class ForegroundEstimateBranch(nn.Module):
    def __init__(self, in_channels: int) -> None:
        super().__init__()
        # 1x1 convolution layer to reduce channels to 1
        self.conv1x1 = nn.Conv2d(in_channels, 1, 1, 1).requires_grad_(False)
    
    def initialize_weights(self, lda: LinearDiscriminantAnalysis, normalizer: MinMaxScaler):
        # Initialize weights and bias using coefficients and intercept from LDA model
        self.conv1x1.weight.data = torch.from_numpy(lda.coef_.T).float()[None, :, :, None] / torch.tensor(normalizer.scale_).float()
        self.conv1x1.bias.data = (torch.from_numpy(lda.intercept_).float() - torch.tensor(normalizer.min_).float()) / torch.tensor(normalizer.scale_).float()
        return self
    
    @torch.no_grad()
    def forward(self, x):
        # Forward pass through the 1x1 convolution layer
        return torch.clamp(self.conv1x1(x), 0, 1)
```
```Python
# 지역 검색 분기
class LocalRetrievalBranch(nn.Module):
    def __init__(self, in_channels_list: List[int], out_channels_list: List[int]) -> None:
        super().__init__()
        self.in_channels_list = in_channels_list
        self.out_channels_list = out_channels_list
        self.conv = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(in_channels, 192, kernel_size=1),
                nn.BatchNorm2d(192),
                nn.ReLU(inplace=True),
                Inception(192, 256),
                nn.Conv2d(256, out_channels, kernel_size=1)
            ) for in_channels, out_channels in zip(in_channels_list, out_channels_list)
        ])
    
    def forward(self, xs: List[torch.Tensor]):
        assert len(xs) == len(self.conv), f'input length must match conv num: {len(xs)} vs {len(self.conv)}'
        return [layer(x) for x, layer in zip(xs, self.conv)]
```
```Python
# 전역 검색 분기
class GlobalRetrievalBranch(nn.Module):
    def __init__(self, bank_size, input_size, in_channels, n_clusters, S, d_method='kl', l_ratio=4/5) -> None:
        super().__init__()
        self.l_ratio = l_ratio
        self.d_method = d_method
        self.codebook = Codebook(n_clusters, in_channels)
        self.block_wise_histogram_encoder = BlockWiseHistogramEncoder(S, input_size, n_clusters)
        self.refs = nn.Parameter(torch.zeros(bank_size, S**2, n_clusters), requires_grad=False)
        self.register_buffer('_bank', torch.tensor(False))

    def initialize_weights(self, kmeans: KMeans):
        self.codebook.initialize_weights(kmeans)
        return self
    
    @torch.no_grad()
    def forward(self, x, return_code = False):
        code = self.codebook(x)
        x = self.block_wise_histogram_encoder(code)
        if return_code:
            return x, code
        return x

    def set_bank(self, refs):
        self.refs.data = refs
        self._bank = torch.tensor(True, device=refs.device)

    def retrieval(self, query):
        assert self._bank, f'GlobalRetrievalBranch must set bank before retrieval.'
        if self.d_method == 'kl':
            idx = torch.argsort(torch.sort(entropy_pytorch(
                        query + 1e-8, self.refs + 1e-8, -1
                    ), -1)[0][:, :int(query.shape[1] * self.l_ratio)].sum(-1))
        else:
            idx = torch.argsort(torch.sort(torch.norm(
                        query - self.refs, dim=-1
                    ), -1)[0][:, :int(query.shape[1] * self.l_ratio)].sum(-1))
        return idx
```
```Python
# DenseNet201 백본 생성
def create_model(model_name: str = 'DenseNet', layers: List[str] = ['features.denseblock1', 'features.denseblock2'], input_size: int = 320, output_dim: int = 384) -> CPR:
    backbone: BaseModel = MODEL_INFOS[model_name]['cls'](layers, input_size=input_size).eval()
    lrb = LocalRetrievalBranch([shape[1] for shape in backbone.feature_extractor.shapes.values()], [output_dim] * len(layers))
    return CPR(backbone, lrb)
```
